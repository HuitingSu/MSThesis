### Test Probability and compare with the probability in paper.
```{r}
library(MASS)
library(glmnet)
```

```{r}
n_rep = 11
n <- 12
p <- 16
l <- 3
q <- 2  # number of true variables
sigma <- 1

X <- read.csv("design1226.csv", skip = 7, header = FALSE)
X <- apply(X, 1, gsub, pattern="\\+", replacement= "1", perl=TRUE)
X <- apply(X, 1, gsub, pattern="\\-", replacement= "-1", perl=TRUE)
X <- matrix(as.numeric(X), n, p)
mt_SC <- logical(n_rep)
y <<- matrix(0, n, l)
b <<- matrix(0, p, l)
true_var <<- numeric(q)
yy <- numeric()
bb <- numeric()
bhat <<- numeric()
```

```{r}
getXy = function(){
    true_var <<- sample(p, q) 
    if(true_var[1] > true_var[2])true_var <<- c(true_var[2], true_var[1])
    b <<- matrix(0, p, l)
    for(k in 1:q){
        for (j in 1:l){
            vsign = sample(c(-1,1), 1)
            b[true_var[k], j] <<- vsign * runif(1, 3, 7)
        }
    }
    y <<- X %*% b
    # epsilon is the gaussian error. dimension: n * l.
    epsilon <- matrix(rnorm(n * l), n, l)
    y <<- y + epsilon
    #return(list(b, y, true_var))
}
```


```{r}
require(parallel)
```

For known combination of true_var, b, lambda, etc, estimate the PDC in multiple reps.
```{r}
estimate_PDC = function(lambda, estimated_A, b_current){
    q = length(estimated_A)
    I = diag(n)
    XA <- X[, estimated_A]
    XC <- X[, -estimated_A]
    bA <- b_current[estimated_A,]
    sgnbA <- sign(bA)
    P = try(solve(t(XA) %*% XA), TRUE)
    if(inherits(P, "try-error"))return(0)
    normb = apply(bA, 1, function(x) sqrt(sum(x^2)))
    std_b = t(sapply(1:q, function(x) bA[x,]/normb[x]))
    lowerbound = 0.9
    
    # eq1 = function(j, ks, V){
    #     #if((sum(ks[j,]>=0) == l || sum(ks[j,]<=0) == l) && max(ks[j,])-min(ks[j,]) < sigma){
    #     std_Vj = V[j,] / sqrt(sum(V[j,]^2))
    #     cos_theta = std_Vj %*% std_b[j,]
    #     if((sum(ks[j,]>=0) == l || sum(ks[j,]<=0) == l) && cos_theta > lowerbou){
    #         if(sum(ks[j,]) >=0) return(TRUE)    ## Their direction is the same
    #         else return(sum(bA[j,]^2) > sum(V[j,]^2)) ## Opposite direction
    #     }
    #     else return(FALSE)
    # }
    
    eq1 = function(j, V){
        std_Vj = V[j,] / sqrt(sum(V[j,]^2))
        cos_theta = std_Vj %*% std_b[j,]
        if(abs(cos_theta) > lowerbound){
            if(cos_theta >=0) return(TRUE)    ## Their direction is the same
            else return(sum(bA[j,]^2) > sum(V[j,]^2)) ## Opposite direction
        }
        else return(FALSE)
    }
    
    eq2 = function(j, E){
        left = t(XC[,j]) %*% (XA %*% P %*% (t(XA)%*%E - n*lambda*std_b) - E )
        left = sqrt(sum(left^2))
        left <= n*lambda
    }
    
    # in this function, return TRUE or FALSE each rep
    calPDC = function(estimated_A, b, lambda, x){
        E <- matrix(mvrnorm(1, rep(0, n*l), diag(n*l)), n, l) 
        
        V = P %*% (t(XA)%*%E - n*lambda*std_b)
        
        equation1 = sapply(1:q, function(j) eq1(j, V))
        eq1_result = sum(equation1) == q
        
        equation2 = sapply(1:dim(XC)[2], function(j) eq2(j, E))
        eq2_result = sum(equation2) == (p-q)
        
        eq1_result && eq2_result
    }
    
    isEventPar = function(estimated_A, b, lambda, trialIndices){
        sapply(1:length(trialIndices), function(x) calPDC(estimated_A, b, lambda, x))
    }
    
    outcomes = pvec(1:10000, function(x) isEventPar(estimated_A, b, lambda, x))
    mean(outcomes)
}
```


```{r}
msv = function(){
    fit = glmnet(X, y, family = "mgaussian", intercept = FALSE)
    attempt <- 0
    max_PDC = 0
    mt_b_best = matrix(0, p, l)
    while(attempt < 1){
        attempt = attempt + 1
        lambda0 = 0
        lambda = rnorm(1, mean=1.9, sd =0.5)
        while(abs(lambda - lambda0) > 0.01){
            #4. Check whether lambda changed
            lambda0 = lambda
            
            #1. estimate beta using an initial lambda
            mt_beta <- matrix(0, p, l)
            for(i in 1:l){
                mt_beta[,i] = eval(parse(text = paste('coef(fit, s=lambda0)$y', i, sep='')))[-1,] 
            }
            estimated_A = which(rowSums(mt_beta) != 0)
            X_Ahat = X[,estimated_A]
            
            #2. OLS estimate regressed only on A 
            estimated_b = apply(y, 2, function(yi) solve(t(X_Ahat) %*% X_Ahat) %*% t(X_Ahat) %*% yi)
            b_current = matrix(0,p,l)
            b_current[estimated_A,] = estimated_b
            
            #3. Calculate PDC and approximate lambda_opt
            result <- optim(lambda, estimate_PDC, method= 'L-BFGS-B', lower = 0, control=list(fnscale=-1), estimated_A = estimated_A, b_current = b_current)
            lambda <- result$par
            value <- result$value
        }

        if(value > max_PDC){
            max_PDC = value
            mt_b_best = mt_beta
        }
    }
    bhat <<- rbind(bhat, mt_b_best)
    return(sum(sign(mt_b_best)==sign(b))==p*l)
}
```


```{r}
set.seed(761313)
for(rep_index in 1:n_rep){
    getXy()
    yy <- rbind(yy, y)
    bb <- rbind(bb, b)
    mt_SC[rep_index] <- msv()
} 
```

```{r}
rep_index = 11
y = yy[((rep_index-1)*12+1):(rep_index*12),]
b = bb[((rep_index-1)*16+1):(rep_index*16),]
mt_SC[rep_index] <- msv()
```


```{r}
mt_cv <- logical(n_rep)
lasso_cv <- logical(n_rep)
lasso_sv <- logical(n_rep)
for(rep_index in 1:n_rep){
    y = yy[((rep_index-1)*12+1):(rep_index*12),]
    b = bb[((rep_index-1)*16+1):(rep_index*16),]
    cv.mt_fit = cv.glmnet(X, y, nfolds = 5, family = "mgaussian", intercept = FALSE)
    mt_beta <- matrix(0, p, l)
    for(i in 1:l){
        mt_beta[,i] = eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep='')))[-1,] 
    }
    mt_cv[rep_index] = sum(sign(mt_beta)==sign(b))==p*l
    
    cv.fit1 = cv.glmnet(X, y[, 1], nfolds = 5, intercept = FALSE)
    cv.fit2 = cv.glmnet(X, y[, 2], nfolds = 5, intercept = FALSE)
    cv.fit3 = cv.glmnet(X, y[, 3], nfolds = 5, intercept = FALSE)
    lasso_beta <- matrix(0, p, l)
    for(i in 1:l){
        lasso_beta[,i] = eval(parse(text = paste('coef(cv.fit', i,')', sep='')))[-1,]
    }
    lasso_cv[rep_index] = sum(sign(lasso_beta)==sign(b))==p*l
}
```


self-voting LASSO
```{r}
n_rep = 20
n <- 12
p <- 16
l <- 3
q <- 2  # number of true variables
sigma <- 1

X <- read.csv("design1226.csv", skip = 7, header = FALSE)
X <- apply(X, 1, gsub, pattern="\\+", replacement= "1", perl=TRUE)
X <- apply(X, 1, gsub, pattern="\\-", replacement= "-1", perl=TRUE)
X <- matrix(as.numeric(X), n, p)

yy <- read.csv("inputy.csv", skip = 1, header = FALSE)
yy <- matrix(unlist(yy), n*20, l)
bb <- read.csv("inputb.csv", header = FALSE)
bb <- matrix(unlist(bb), p*20, l)
```

```{r}
for(rep_index in 6:n_rep){
    y = yy[((rep_index-1)*12+1):(rep_index*12),]
    b = bb[((rep_index-1)*16+1):(rep_index*16),]    
    sv_lasso_beta = cbind(svlasso(1), svlasso(2), svlasso(3)) 
    lasso_sv[rep_index] = sum(sign(sv_lasso_beta)==sign(b))==p*l
}
```


self-voting lasso
```{r}
estimate_PSC <- function(true_var, b, lambda){
    n_sample = 30000
    q = length(true_var)
    I = diag(n)
    XA <- X[, true_var]
    XC <- X[, -true_var]
    bA <- b[true_var]
    sgnbA <- sign(bA)
    inv = solve(t(XA) %*% XA)
    
    R <- t(XC) %*% XA %*% inv %*% sgnbA
    P <- XA %*% inv %*% t(XA)
#    D <- lambda/2 * inv %*% sgnbA
    D <- lambda*n * inv %*% sgnbA
    
#    U <- mvrnorm(n = n_sample, R, 4*sigma^2/lambda^2 * t(XC) %*% (I-P) %*% t(I-P) %*% XC)
#    V <- mvrnorm(n = n_sample, D, sigma^2 * inv %*% t(XA) %*% XA %*% t(inv))  # the later is I
    
    U <- mvrnorm(n = n_sample, R, sigma^2/(lambda^2 * n^2) * t(XC) %*% (I-P) %*% t(I-P) %*% XC)
    V <- mvrnorm(n = n_sample, D, sigma^2 * inv %*% t(XA) %*% XA %*% t(inv))  # the later is I
    
    P1 <- U >= -1 & U <= 1   # a vector showing every rep
    P1 <- rowSums(P1) == (p - q)
    Vpos = matrix(sapply(V,max,0),n_sample, q)
    Vneg = matrix(sapply(V,min,0),n_sample, q)
    P2 <- rowSums(bA >= Vpos | bA <= Vneg) == q
    
    PSC = sum(P1 & P2)/n_sample
    return(PSC)
}
```

```{r}
svlasso = function(k){
    max_PSC <- 0
    fit = glmnet(X, y[,k], intercept = FALSE)
    lambda0 = 0
    lambda = rnorm(1, mean=2, sd =1)
    while(abs(lambda - lambda0) > 0.01){
        lambda0 = lambda
        b_lasso = coef(fit, s = lambda0)[-1]
        estimated_A = which(b_lasso != 0)
        X_Ahat = X[,estimated_A]
        estimated_b = solve(t(X_Ahat) %*% X_Ahat) %*% t(X_Ahat) %*% y[,k]
        b_current = numeric(p)
        b_current[estimated_A] = estimated_b
        result <- optim(lambda, estimate_PSC, method= 'L-BFGS-B', lower = 0, true_var = estimated_A, b = b_current)
        lambda <- result$par
        value <- result$value
    }
    b_lasso = coef(fit, s = lambda0)[-1]
    return(b_lasso)
}
```



```{r}
PSC = sapply(seq(0.1,8,by=0.1), estimate_PSC, true_var = c(12,14), b = b[,1])
plot(PSC)
```

```{r}
sapply(seq(1.5,1.5,by=0.3), estimate_PDC, estimated_A = true_var, b_current = b)
```

```{r}
cvfit = cv.glmnet(X, y, intercept = FALSE)
plot(cvfit)
coef(cvfit, s =2)
```











