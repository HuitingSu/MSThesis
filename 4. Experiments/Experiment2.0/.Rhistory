#library(MASS)
library(glmnet)
#library(MASS)
library(glmnet)
n_rep <- 100
a <- 5  # absolute value of true factor
SameTrue <- TRUE
#-------------------------
n <- 12
p <- 16
l <- 5
q <- 2  # number of true variables
X <- read.csv("design1226.csv", skip = 7, header = FALSE)
X <- apply(X, 1, gsub, pattern="\\+", replacement= "1", perl=TRUE)
X <- apply(X, 1, gsub, pattern="\\-", replacement= "-1", perl=TRUE)
X <- matrix(as.numeric(X), n, p)
lasso_OC <<- numeric()
mt_OC <<- numeric()
lasso_SC <<- numeric()
mt_SC <<- numeric()
lasso_OC2 <<- numeric()
mt_OC2 <<- numeric()
lasso_SC2 <<- numeric()
mt_SC2 <<- numeric()
getXy <- function(){
# Have 5 different responses
# There are 3 same true var for different tasks, the true vars are randomly chosen
# for every task, we need a vector to set the true vars
w <<- matrix(0, p, l)
if(SameTrue == TRUE){
true_var <- sample(p, q)
for (j in 1:l)
w[true_var, j] <<- a  #runif(3, 3, 5)
}else{
for (j in 1:l){
true_var <- sample(p, q)
w[true_var, j] <<- a  #runif(3, 3, 5)
}
}
y <<- X %*% w
# epsilon is the gaussian error. dimension: n * l.
epsilon <- matrix(rnorm(n * l), n, l)
y <<- y + 0.1*epsilon
}
compare <- function() {
mt_fit = glmnet(X, y, family = "mgaussian")
#plot(mt_fit, type.coef = "2norm", label = TRUE)
## Use Cross Validation to select $\lambda$, and compare.
cv.mt_fit = cv.glmnet(X, y, family = "mgaussian", intercept = FALSE)
cv.fit1 = cv.glmnet(X, y[, 1], intercept = FALSE)
cv.fit2 = cv.glmnet(X, y[, 2], intercept = FALSE)
cv.fit3 = cv.glmnet(X, y[, 3], intercept = FALSE)
cv.fit4 = cv.glmnet(X, y[, 4], intercept = FALSE)
cv.fit5 = cv.glmnet(X, y[, 5], intercept = FALSE)
# Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold
# If grouped=FALSE, an error matrix is built up at the observation level from the predictions from the nfold fits, and then summarized
#plot(cv.mt_fit)
#plot(cv.fit1)
#plot(cv.fit2)
#plot(cv.fit3)
# multi-task
mt_beta = numeric()
for(i in 1:l){
mt_beta = cbind(mt_beta,  eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep=''))) )
}
mt_beta = mt_beta[-1,]
#####print(mt_beta)
# lasso for each response
lasso_beta = numeric()
for(i in 1:l){
lasso_beta = cbind(lasso_beta,  eval(parse(text = paste('coef(cv.fit', i,')', sep=''))) )
}
lasso_beta = lasso_beta[-1,]
####print(lasso_beta)
# get whether correct selection
mt_OC <<- c(mt_OC, sum(as.logical(w) == as.logical(mt_beta)) == p*l )
lasso_OC <<- c(lasso_OC, sum(as.logical(w) == as.logical(lasso_beta)) == p * l)
mt_SC <<- c(mt_SC, sum(sign(w) == sign(mt_beta)) == p*l )
lasso_SC <<- c(lasso_SC, sum(sign(w) == sign(lasso_beta)) == p*l )
#
mt_OC2 <<- c(mt_OC2, sum(as.logical(w) == as.logical(mt_beta)) / (p * l))
lasso_OC2 <<- c(lasso_OC2, sum(as.logical(w) == as.logical(lasso_beta)) / (p * l))
mt_SC2 <<- c(mt_SC2, sum(sign(w) == sign(mt_beta)) / (p * l))
lasso_SC2 <<- c(lasso_SC2, sum(sign(w) == sign(lasso_beta)) / (p * l))
}
set.seed(87293)
for(ii in 1:n_rep){
getXy()
compare()
}
#print(mt_OC)
#print(lasso_OC)
cat(mean(mt_OC), mean(lasso_OC), mean(mt_SC), mean(lasso_SC), mean(mt_OC2), mean(lasso_OC2), mean(mt_SC2), mean(lasso_SC2))
cat(
rbind(
t.test(mt_OC)$conf.int[1:2],
t.test(lasso_OC)$conf.int[1:2],
t.test(mt_SC)$conf.int[1:2],
t.test(lasso_SC)$conf.int[1:2],
t.test(mt_OC2)$conf.int[1:2],
t.test(lasso_OC2)$conf.int[1:2],
t.test(mt_SC2)$conf.int[1:2],
t.test(lasso_SC2)$conf.int[1:2])
)
cat(mean(mt_OC), mean(lasso_OC), mean(mt_SC), mean(lasso_SC), mean(mt_OC2), mean(lasso_OC2), mean(mt_SC2), mean(lasso_SC2),'\n')
cat(
rbind(
t.test(mt_OC)$conf.int[1:2],
t.test(lasso_OC)$conf.int[1:2],
t.test(mt_SC)$conf.int[1:2],
t.test(lasso_SC)$conf.int[1:2],
t.test(mt_OC2)$conf.int[1:2],
t.test(lasso_OC2)$conf.int[1:2],
t.test(mt_SC2)$conf.int[1:2],
t.test(lasso_SC2)$conf.int[1:2])
)
t.test(mt_OC)$conf.int[1:2]
print(
cbind(
t.test(mt_OC)$conf.int[1:2],
t.test(lasso_OC)$conf.int[1:2],
t.test(mt_SC)$conf.int[1:2],
t.test(lasso_SC)$conf.int[1:2],
t.test(mt_OC2)$conf.int[1:2],
t.test(lasso_OC2)$conf.int[1:2],
t.test(mt_SC2)$conf.int[1:2],
t.test(lasso_SC2)$conf.int[1:2])
)
cat(mean(mt_OC), mean(lasso_OC), mean(mt_SC), mean(lasso_SC), mean(mt_OC2), mean(lasso_OC2), mean(mt_SC2), mean(lasso_SC2),'\n')
n_rep <- 500
a <- 5  # absolute value of true factor
SameTrue <- TRUE
#-------------------------
n <- 12
p <- 16
l <- 5
q <- 2  # number of true variables
X <- read.csv("design1226.csv", skip = 7, header = FALSE)
X <- apply(X, 1, gsub, pattern="\\+", replacement= "1", perl=TRUE)
X <- apply(X, 1, gsub, pattern="\\-", replacement= "-1", perl=TRUE)
X <- matrix(as.numeric(X), n, p)
lasso_OC <<- numeric()
mt_OC <<- numeric()
lasso_SC <<- numeric()
mt_SC <<- numeric()
lasso_OC2 <<- numeric()
mt_OC2 <<- numeric()
lasso_SC2 <<- numeric()
mt_SC2 <<- numeric()
set.seed(87293)
for(ii in 1:n_rep){
getXy()
compare()
}
n_rep <- 200 #500 rep is quite long
a <- 5  # absolute value of true factor
SameTrue <- TRUE
#-------------------------
n <- 12
p <- 16
l <- 5
q <- 2  # number of true variables
X <- read.csv("design1226.csv", skip = 7, header = FALSE)
X <- apply(X, 1, gsub, pattern="\\+", replacement= "1", perl=TRUE)
X <- apply(X, 1, gsub, pattern="\\-", replacement= "-1", perl=TRUE)
X <- matrix(as.numeric(X), n, p)
lasso_OC <<- numeric()
mt_OC <<- numeric()
lasso_SC <<- numeric()
mt_SC <<- numeric()
lasso_OC2 <<- numeric()
mt_OC2 <<- numeric()
lasso_SC2 <<- numeric()
mt_SC2 <<- numeric()
getXy <- function(){
# Have 5 different responses
# There are 3 same true var for different tasks, the true vars are randomly chosen
# for every task, we need a vector to set the true vars
w <<- matrix(0, p, l)
if(SameTrue == TRUE){
true_var <- sample(p, q)
for (j in 1:l)
w[true_var, j] <<- a  #runif(3, 3, 5)
}else{
for (j in 1:l){
true_var <- sample(p, q)
w[true_var, j] <<- a  #runif(3, 3, 5)
}
}
y <<- X %*% w
# epsilon is the gaussian error. dimension: n * l.
epsilon <- matrix(rnorm(n * l), n, l)
y <<- y + 0.1*epsilon
}
compare <- function() {
mt_fit = glmnet(X, y, family = "mgaussian")
#plot(mt_fit, type.coef = "2norm", label = TRUE)
## Use Cross Validation to select $\lambda$, and compare.
cv.mt_fit = cv.glmnet(X, y, family = "mgaussian", intercept = FALSE)
cv.fit1 = cv.glmnet(X, y[, 1], intercept = FALSE)
cv.fit2 = cv.glmnet(X, y[, 2], intercept = FALSE)
cv.fit3 = cv.glmnet(X, y[, 3], intercept = FALSE)
cv.fit4 = cv.glmnet(X, y[, 4], intercept = FALSE)
cv.fit5 = cv.glmnet(X, y[, 5], intercept = FALSE)
# Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold
# If grouped=FALSE, an error matrix is built up at the observation level from the predictions from the nfold fits, and then summarized
#plot(cv.mt_fit)
#plot(cv.fit1)
#plot(cv.fit2)
#plot(cv.fit3)
# multi-task
mt_beta = numeric()
for(i in 1:l){
mt_beta = cbind(mt_beta,  eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep=''))) )
}
mt_beta = mt_beta[-1,]
#####print(mt_beta)
# lasso for each response
lasso_beta = numeric()
for(i in 1:l){
lasso_beta = cbind(lasso_beta,  eval(parse(text = paste('coef(cv.fit', i,')', sep=''))) )
}
lasso_beta = lasso_beta[-1,]
####print(lasso_beta)
# get whether correct selection
mt_OC <<- c(mt_OC, sum(as.logical(w) == as.logical(mt_beta)) == p*l )
lasso_OC <<- c(lasso_OC, sum(as.logical(w) == as.logical(lasso_beta)) == p * l)
mt_SC <<- c(mt_SC, sum(sign(w) == sign(mt_beta)) == p*l )
lasso_SC <<- c(lasso_SC, sum(sign(w) == sign(lasso_beta)) == p*l )
#
mt_OC2 <<- c(mt_OC2, sum(as.logical(w) == as.logical(mt_beta)) / (p * l))
lasso_OC2 <<- c(lasso_OC2, sum(as.logical(w) == as.logical(lasso_beta)) / (p * l))
mt_SC2 <<- c(mt_SC2, sum(sign(w) == sign(mt_beta)) / (p * l))
lasso_SC2 <<- c(lasso_SC2, sum(sign(w) == sign(lasso_beta)) / (p * l))
}
set.seed(87293)
for(ii in 1:n_rep){
getXy()
compare()
}
View(y)
#print(mt_OC)
#print(lasso_OC)
cat(mean(mt_OC), mean(lasso_OC), mean(mt_SC), mean(lasso_SC), mean(mt_OC2), mean(lasso_OC2), mean(mt_SC2), mean(lasso_SC2),'\n')
print(
cbind(
t.test(mt_OC)$conf.int[1:2],
t.test(lasso_OC)$conf.int[1:2],
t.test(mt_SC)$conf.int[1:2],
t.test(lasso_SC)$conf.int[1:2],
t.test(mt_OC2)$conf.int[1:2],
t.test(lasso_OC2)$conf.int[1:2],
t.test(mt_SC2)$conf.int[1:2],
t.test(lasso_SC2)$conf.int[1:2])
)
cat(mean(mt_OC), mean(lasso_OC), mean(mt_SC), mean(lasso_SC), mean(mt_OC2), mean(lasso_OC2), mean(mt_SC2), mean(lasso_SC2),'\n')
print(
cbind(
t.test(mt_OC)$conf.int[1:2],
t.test(lasso_OC)$conf.int[1:2],
t.test(mt_SC)$conf.int[1:2],
t.test(lasso_SC)$conf.int[1:2],
t.test(mt_OC2)$conf.int[1:2],
t.test(lasso_OC2)$conf.int[1:2],
t.test(mt_SC2)$conf.int[1:2],
t.test(lasso_SC2)$conf.int[1:2])
)
View(y)
getXy <- function(){
# Have 5 different responses
# There are 3 same true var for different tasks, the true vars are randomly chosen
# for every task, we need a vector to set the true vars
w <<- matrix(0, p, l)
if(SameTrue == TRUE){
true_var <- sample(p, q)
for (j in 1:l)
w[true_var, j] <<- a  #runif(3, 3, 5)
}else{
for (j in 1:l){
true_var <- sample(p, q)
w[true_var, j] <<- a  #runif(3, 3, 5)
}
}
y <<- X %*% w
# epsilon is the gaussian error. dimension: n * l.
epsilon <- matrix(rnorm(n * l), n, l)
y <<- y + epsilon
}
n_rep <- 100 #500 rep is quite long
a <- 5  # absolute value of true factor
SameTrue <- TRUE
#-------------------------
n <- 12
p <- 16
l <- 5
q <- 2  # number of true variables
X <- read.csv("design1226.csv", skip = 7, header = FALSE)
X <- apply(X, 1, gsub, pattern="\\+", replacement= "1", perl=TRUE)
X <- apply(X, 1, gsub, pattern="\\-", replacement= "-1", perl=TRUE)
X <- matrix(as.numeric(X), n, p)
lasso_OC <<- numeric()
mt_OC <<- numeric()
lasso_SC <<- numeric()
mt_SC <<- numeric()
lasso_OC2 <<- numeric()
mt_OC2 <<- numeric()
lasso_SC2 <<- numeric()
mt_SC2 <<- numeric()
#library(MASS)
library(glmnet)
#library(MASS)
library(glmnet)
n_rep <- 100 #500 rep is quite long
a <- 5  # absolute value of true factor
SameTrue <- TRUE
#-------------------------
n <- 12
p <- 16
l <- 5
q <- 2  # number of true variables
X <- read.csv("design1226.csv", skip = 7, header = FALSE)
X <- apply(X, 1, gsub, pattern="\\+", replacement= "1", perl=TRUE)
X <- apply(X, 1, gsub, pattern="\\-", replacement= "-1", perl=TRUE)
X <- matrix(as.numeric(X), n, p)
lasso_OC <<- numeric()
mt_OC <<- numeric()
lasso_SC <<- numeric()
mt_SC <<- numeric()
lasso_OC2 <<- numeric()
mt_OC2 <<- numeric()
lasso_SC2 <<- numeric()
mt_SC2 <<- numeric()
getXy <- function(){
# Have 5 different responses
# There are 3 same true var for different tasks, the true vars are randomly chosen
# for every task, we need a vector to set the true vars
w <<- matrix(0, p, l)
if(SameTrue == TRUE){
true_var <- sample(p, q)
for (j in 1:l)
w[true_var, j] <<- a  #runif(3, 3, 5)
}else{
for (j in 1:l){
true_var <- sample(p, q)
w[true_var, j] <<- a  #runif(3, 3, 5)
}
}
y <<- X %*% w
# epsilon is the gaussian error. dimension: n * l.
epsilon <- matrix(rnorm(n * l), n, l)
y <<- y + epsilon
}
compare <- function() {
mt_fit = glmnet(X, y, family = "mgaussian")
#plot(mt_fit, type.coef = "2norm", label = TRUE)
## Use Cross Validation to select $\lambda$, and compare.
cv.mt_fit = cv.glmnet(X, y, family = "mgaussian", intercept = FALSE)
cv.fit1 = cv.glmnet(X, y[, 1], intercept = FALSE)
cv.fit2 = cv.glmnet(X, y[, 2], intercept = FALSE)
cv.fit3 = cv.glmnet(X, y[, 3], intercept = FALSE)
cv.fit4 = cv.glmnet(X, y[, 4], intercept = FALSE)
cv.fit5 = cv.glmnet(X, y[, 5], intercept = FALSE)
# Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold
# If grouped=FALSE, an error matrix is built up at the observation level from the predictions from the nfold fits, and then summarized
#plot(cv.mt_fit)
#plot(cv.fit1)
#plot(cv.fit2)
#plot(cv.fit3)
# multi-task
mt_beta = numeric()
for(i in 1:l){
mt_beta = cbind(mt_beta,  eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep=''))) )
}
mt_beta = mt_beta[-1,]
#####print(mt_beta)
# lasso for each response
lasso_beta = numeric()
for(i in 1:l){
lasso_beta = cbind(lasso_beta,  eval(parse(text = paste('coef(cv.fit', i,')', sep=''))) )
}
lasso_beta = lasso_beta[-1,]
####print(lasso_beta)
# get whether correct selection
mt_OC <<- c(mt_OC, sum(as.logical(w) == as.logical(mt_beta)) == p*l )
lasso_OC <<- c(lasso_OC, sum(as.logical(w) == as.logical(lasso_beta)) == p * l)
mt_SC <<- c(mt_SC, sum(sign(w) == sign(mt_beta)) == p*l )
lasso_SC <<- c(lasso_SC, sum(sign(w) == sign(lasso_beta)) == p*l )
#
mt_OC2 <<- c(mt_OC2, sum(as.logical(w) == as.logical(mt_beta)) / (p * l))
lasso_OC2 <<- c(lasso_OC2, sum(as.logical(w) == as.logical(lasso_beta)) / (p * l))
mt_SC2 <<- c(mt_SC2, sum(sign(w) == sign(mt_beta)) / (p * l))
lasso_SC2 <<- c(lasso_SC2, sum(sign(w) == sign(lasso_beta)) / (p * l))
}
set.seed(87293)
for(ii in 1:n_rep){
getXy()
compare()
}
#print(mt_OC)
#print(lasso_OC)
cat(mean(mt_OC), mean(lasso_OC), mean(mt_SC), mean(lasso_SC), mean(mt_OC2), mean(lasso_OC2), mean(mt_SC2), mean(lasso_SC2),'\n')
print(
cbind(
t.test(mt_OC)$conf.int[1:2],
t.test(lasso_OC)$conf.int[1:2],
t.test(mt_SC)$conf.int[1:2],
t.test(lasso_SC)$conf.int[1:2],
t.test(mt_OC2)$conf.int[1:2],
t.test(lasso_OC2)$conf.int[1:2],
t.test(mt_SC2)$conf.int[1:2],
t.test(lasso_SC2)$conf.int[1:2])
)
cat(mean(mt_OC), mean(lasso_OC), mean(mt_SC), mean(lasso_SC), mean(mt_OC2), mean(lasso_OC2), mean(mt_SC2), mean(lasso_SC2),'\n')
