#plot(cv.fit3)
# multi-task
mt_beta = cbind(coef(cv.mt_fit)$y1, coef(cv.mt_fit)$y2, coef(cv.mt_fit)$y3, coef(cv.mt_fit)$y4, coef(cv.mt_fit)$y5)
mt_beta = mt_beta[-1, ]
#print(mt_beta)
#print(sum((mt_beta - w) ^ 2))
# lasso for each response
lasso_beta = cbind(coef(cv.fit1), coef(cv.fit2), coef(cv.fit3), coef(cv.fit4), coef(cv.fit5))
lasso_beta = lasso_beta[-1, ]
#print(lasso_beta)
#print(sum((lasso_beta - w) ^ 2))
# get whether correct selection
lasso_correct <- c(lasso_correct, sum(as.logical(w) == as.logical(lasso_beta))/(p*l) )
mt_correct <- c(mt_correct, sum(as.logical(w) == as.logical(mt_beta))/(p*l) )
(w - lasso_beta)^2
w - lasso_beta
sum((w - lasso_beta)^2)
# (n=12, p=16) q in [1,3] 3, l = 5
n <- 12
p <- 16
l <- 5
q <- 3
lasso_correct <- numeric()
mt_correct <- numeric()
lasso_wRSS <- numeric()
mt_wRSS <- numeric()
X <- read.csv("design1226.csv", skip = 7, header = FALSE)
X <- apply(X, 1, gsub, pattern="\\+", replacement= "1", perl=TRUE)
X <- apply(X, 1, gsub, pattern="\\-", replacement= "-1", perl=TRUE)
X <- matrix(as.numeric(X), n, p)
# Have 5 different responses
# There are 3 same true var for different tasks, the true vars are randomly chosen
set.seed(65587)
# for every task, we need a vector to set the true vars
w <- matrix(0, p, l)
for(j in 1:l){
true_var <- c(1,2,3)  #sample(p, q)
w[true_var, j] = round(rnorm(q, mean = 0, sd = 1), 3)
}
y = X %*% w
set.seed(12345)
# epsilon is the gaussian error. dimension: n * l.
epsilon <- matrix(rnorm(n*l), n, l)
y <- y + epsilon
mt_fit = glmnet(X, y, family = "mgaussian")
#plot(mt_fit, type.coef = "2norm", label = TRUE)
## Use Cross Validation to select $\lambda$, and compare.
cv.mt_fit = cv.glmnet(X, y, family = "mgaussian", intercept = FALSE)
cv.fit1 = cv.glmnet(X, y[, 1], intercept = FALSE)
cv.fit2 = cv.glmnet(X, y[, 2], intercept = FALSE)
cv.fit3 = cv.glmnet(X, y[, 3], intercept = FALSE)
cv.fit4 = cv.glmnet(X, y[, 4], intercept = FALSE)
cv.fit5 = cv.glmnet(X, y[, 5], intercept = FALSE)
# Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold
# If grouped=FALSE, an error matrix is built up at the observation level from the predictions from the nfold fits, and then summarized
#plot(cv.mt_fit)
#plot(cv.fit1)
#plot(cv.fit2)
#plot(cv.fit3)
# multi-task
mt_beta = cbind(coef(cv.mt_fit)$y1, coef(cv.mt_fit)$y2, coef(cv.mt_fit)$y3, coef(cv.mt_fit)$y4, coef(cv.mt_fit)$y5)
mt_beta = mt_beta[-1, ]
#print(mt_beta)
#print(sum((mt_beta - w) ^ 2))
# lasso for each response
lasso_beta = cbind(coef(cv.fit1), coef(cv.fit2), coef(cv.fit3), coef(cv.fit4), coef(cv.fit5))
lasso_beta = lasso_beta[-1, ]
#print(lasso_beta)
#print(sum((lasso_beta - w) ^ 2))
# get whether correct selection
mt_correct <- c(mt_correct, sum(as.logical(w) == as.logical(mt_beta))/(p*l) )
lasso_correct <- c(lasso_correct, sum(as.logical(w) == as.logical(lasso_beta))/(p*l) )
# get RSS fotr w
lasso_wRSS <- c(lasso_wRSS, sum((w - lasso_beta)^2) )
mt_wRSS <- c(mt_wRSS, sum((w - mt_beta)^2) )
# (n=12, p=16) q in [1,3] 3, l = 5
n <- 12
p <- 16
l <- 5
q <- 3
X <- read.csv("design1226.csv", skip = 7, header = FALSE)
X <- apply(X, 1, gsub, pattern="\\+", replacement= "1", perl=TRUE)
X <- apply(X, 1, gsub, pattern="\\-", replacement= "-1", perl=TRUE)
X <- matrix(as.numeric(X), n, p)
# (n=12, p=16) q in [1,3] 3, l = 5
n <- 12
p <- 16
l <- 5
q <- 3
X <- read.csv("design1226.csv", skip = 7, header = FALSE)
X <- apply(X, 1, gsub, pattern="\\+", replacement= "1", perl=TRUE)
X <- apply(X, 1, gsub, pattern="\\-", replacement= "-1", perl=TRUE)
X <- matrix(as.numeric(X), n, p)
getXy <- function(){
lasso_correct <- numeric()
mt_correct <- numeric()
lasso_wRSS <- numeric()
mt_wRSS <- numeric()
# Have 5 different responses
# There are 3 same true var for different tasks, the true vars are randomly chosen
set.seed(65587)
# for every task, we need a vector to set the true vars
w <- matrix(0, p, l)
for (j in 1:l) {
true_var <- c(1, 2, 3)  #sample(p, q)
w[true_var, j] = round(rnorm(q, mean = 0, sd = 1), 3)
}
y = X %*% w
set.seed(12345)
# epsilon is the gaussian error. dimension: n * l.
epsilon <- matrix(rnorm(n * l), n, l)
y <- y + epsilon
}
compare <- function() {
mt_fit = glmnet(X, y, family = "mgaussian")
#plot(mt_fit, type.coef = "2norm", label = TRUE)
## Use Cross Validation to select $\lambda$, and compare.
cv.mt_fit = cv.glmnet(X, y, family = "mgaussian", intercept = FALSE)
cv.fit1 = cv.glmnet(X, y[, 1], intercept = FALSE)
cv.fit2 = cv.glmnet(X, y[, 2], intercept = FALSE)
cv.fit3 = cv.glmnet(X, y[, 3], intercept = FALSE)
cv.fit4 = cv.glmnet(X, y[, 4], intercept = FALSE)
cv.fit5 = cv.glmnet(X, y[, 5], intercept = FALSE)
# Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold
# If grouped=FALSE, an error matrix is built up at the observation level from the predictions from the nfold fits, and then summarized
#plot(cv.mt_fit)
#plot(cv.fit1)
#plot(cv.fit2)
#plot(cv.fit3)
# multi-task
mt_beta = cbind(
coef(cv.mt_fit)$y1,
coef(cv.mt_fit)$y2,
coef(cv.mt_fit)$y3,
coef(cv.mt_fit)$y4,
coef(cv.mt_fit)$y5
)
mt_beta = mt_beta[-1,]
#print(mt_beta)
#print(sum((mt_beta - w) ^ 2))
# lasso for each response
lasso_beta = cbind(coef(cv.fit1),
coef(cv.fit2),
coef(cv.fit3),
coef(cv.fit4),
coef(cv.fit5))
lasso_beta = lasso_beta[-1,]
#print(lasso_beta)
#print(sum((lasso_beta - w) ^ 2))
# get whether correct selection
mt_correct <-
c(mt_correct, sum(as.logical(w) == as.logical(mt_beta)) / (p * l))
lasso_correct <-
c(lasso_correct, sum(as.logical(w) == as.logical(lasso_beta)) / (p * l))
# get RSS fotr w
lasso_wRSS <- c(lasso_wRSS, sum((w - lasso_beta) ^ 2))
mt_wRSS <- c(mt_wRSS, sum((w - mt_beta) ^ 2))
}
for(ii in 1:10000){
getXy()
compare()
}
getXy()
getXy <- function(){
lasso_correct <<- numeric()
mt_correct <<- numeric()
lasso_wRSS <<- numeric()
mt_wRSS <<- numeric()
# Have 5 different responses
# There are 3 same true var for different tasks, the true vars are randomly chosen
set.seed(65587)
# for every task, we need a vector to set the true vars
w <<- matrix(0, p, l)
for (j in 1:l) {
true_var <- c(1, 2, 3)  #sample(p, q)
w[true_var, j] <- round(rnorm(q, mean = 0, sd = 1), 3)
}
y <<- X %*% w
set.seed(12345)
# epsilon is the gaussian error. dimension: n * l.
epsilon <- matrix(rnorm(n * l), n, l)
y <<- y + epsilon
}
getXy()
compare <- function() {
mt_fit = glmnet(X, y, family = "mgaussian")
#plot(mt_fit, type.coef = "2norm", label = TRUE)
## Use Cross Validation to select $\lambda$, and compare.
cv.mt_fit = cv.glmnet(X, y, family = "mgaussian", intercept = FALSE)
cv.fit1 = cv.glmnet(X, y[, 1], intercept = FALSE)
cv.fit2 = cv.glmnet(X, y[, 2], intercept = FALSE)
cv.fit3 = cv.glmnet(X, y[, 3], intercept = FALSE)
cv.fit4 = cv.glmnet(X, y[, 4], intercept = FALSE)
cv.fit5 = cv.glmnet(X, y[, 5], intercept = FALSE)
# Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold
# If grouped=FALSE, an error matrix is built up at the observation level from the predictions from the nfold fits, and then summarized
#plot(cv.mt_fit)
#plot(cv.fit1)
#plot(cv.fit2)
#plot(cv.fit3)
# multi-task
mt_beta = cbind(
coef(cv.mt_fit)$y1,
coef(cv.mt_fit)$y2,
coef(cv.mt_fit)$y3,
coef(cv.mt_fit)$y4,
coef(cv.mt_fit)$y5
)
mt_beta = mt_beta[-1,]
#print(mt_beta)
#print(sum((mt_beta - w) ^ 2))
# lasso for each response
lasso_beta = cbind(coef(cv.fit1),
coef(cv.fit2),
coef(cv.fit3),
coef(cv.fit4),
coef(cv.fit5))
lasso_beta = lasso_beta[-1,]
#print(lasso_beta)
#print(sum((lasso_beta - w) ^ 2))
# get whether correct selection
mt_correct <<-
c(mt_correct, sum(as.logical(w) == as.logical(mt_beta)) / (p * l))
lasso_correct <<-
c(lasso_correct, sum(as.logical(w) == as.logical(lasso_beta)) / (p * l))
# get RSS fotr w
lasso_wRSS <- c(lasso_wRSS, sum((w - lasso_beta) ^ 2))
mt_wRSS <- c(mt_wRSS, sum((w - mt_beta) ^ 2))
}
for(ii in 1:10000){
getXy()
compare()
}
for(ii in 1:10){
getXy()
compare()
}
# (n=12, p=16) q in [1,3] 3, l = 5
n <- 12
p <- 16
l <- 5
q <- 3
X <- read.csv("design1226.csv", skip = 7, header = FALSE)
X <- apply(X, 1, gsub, pattern="\\+", replacement= "1", perl=TRUE)
X <- apply(X, 1, gsub, pattern="\\-", replacement= "-1", perl=TRUE)
X <- matrix(as.numeric(X), n, p)
getXy <- function(){
lasso_correct <<- numeric()
mt_correct <<- numeric()
lasso_wRSS <<- numeric()
mt_wRSS <<- numeric()
# Have 5 different responses
# There are 3 same true var for different tasks, the true vars are randomly chosen
set.seed(65587)
# for every task, we need a vector to set the true vars
w <<- matrix(0, p, l)
for (j in 1:l) {
true_var <- c(1, 2, 3)  #sample(p, q)
w[true_var, j] <<- round(rnorm(q, mean = 0, sd = 1), 3)
}
y <<- X %*% w
set.seed(12345)
# epsilon is the gaussian error. dimension: n * l.
epsilon <- matrix(rnorm(n * l), n, l)
y <<- y + epsilon
}
sdf <- numeric()
sdf <- c(sdf, 1)
sdf
sdf <- c(sdf, 1)
sdf
compare <- function() {
mt_fit = glmnet(X, y, family = "mgaussian")
#plot(mt_fit, type.coef = "2norm", label = TRUE)
## Use Cross Validation to select $\lambda$, and compare.
cv.mt_fit = cv.glmnet(X, y, family = "mgaussian", intercept = FALSE)
cv.fit1 = cv.glmnet(X, y[, 1], intercept = FALSE)
cv.fit2 = cv.glmnet(X, y[, 2], intercept = FALSE)
cv.fit3 = cv.glmnet(X, y[, 3], intercept = FALSE)
cv.fit4 = cv.glmnet(X, y[, 4], intercept = FALSE)
cv.fit5 = cv.glmnet(X, y[, 5], intercept = FALSE)
# Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold
# If grouped=FALSE, an error matrix is built up at the observation level from the predictions from the nfold fits, and then summarized
#plot(cv.mt_fit)
#plot(cv.fit1)
#plot(cv.fit2)
#plot(cv.fit3)
# multi-task
mt_beta = cbind(
coef(cv.mt_fit)$y1,
coef(cv.mt_fit)$y2,
coef(cv.mt_fit)$y3,
coef(cv.mt_fit)$y4,
coef(cv.mt_fit)$y5
)
mt_beta = mt_beta[-1,]
#print(mt_beta)
#print(sum((mt_beta - w) ^ 2))
# lasso for each response
lasso_beta = cbind(coef(cv.fit1),
coef(cv.fit2),
coef(cv.fit3),
coef(cv.fit4),
coef(cv.fit5))
lasso_beta = lasso_beta[-1,]
#print(lasso_beta)
#print(sum((lasso_beta - w) ^ 2))
# get whether correct selection
mt_correct <<-
c(mt_correct, sum(as.logical(w) == as.logical(mt_beta)) / (p * l))
lasso_correct <<-
c(lasso_correct, sum(as.logical(w) == as.logical(lasso_beta)) / (p * l))
# get RSS fotr w
lasso_wRSS <<- c(lasso_wRSS, sum((w - lasso_beta) ^ 2))
mt_wRSS <<- c(mt_wRSS, sum((w - mt_beta) ^ 2))
}
for(ii in 1:10){
getXy()
compare()
}
lasso_correct
lasso_wRSS <<- c(lasso_wRSS, sum((w - lasso_beta) ^ 2))
mt_fit = glmnet(X, y, family = "mgaussian")
cv.mt_fit = cv.glmnet(X, y, family = "mgaussian", intercept = FALSE)
mt_beta = cbind(
coef(cv.mt_fit)$y1,
coef(cv.mt_fit)$y2,
coef(cv.mt_fit)$y3,
coef(cv.mt_fit)$y4,
coef(cv.mt_fit)$y5
)
mt_beta = mt_beta[-1,]
mt_correct <<-
c(mt_correct, sum(as.logical(w) == as.logical(mt_beta)) / (p * l))
mt_wRSS <<- c(mt_wRSS, sum((w - mt_beta) ^ 2))
set.seed(65587)
# (n=12, p=16) q in [1,3] 3, l = 5
n <- 12
p <- 16
l <- 5
q <- 3
X <- read.csv("design1226.csv", skip = 7, header = FALSE)
X <- apply(X, 1, gsub, pattern="\\+", replacement= "1", perl=TRUE)
X <- apply(X, 1, gsub, pattern="\\-", replacement= "-1", perl=TRUE)
X <- matrix(as.numeric(X), n, p)
getXy <- function(){
lasso_correct <<- numeric()
mt_correct <<- numeric()
lasso_wRSS <<- numeric()
mt_wRSS <<- numeric()
# Have 5 different responses
# There are 3 same true var for different tasks, the true vars are randomly chosen
# for every task, we need a vector to set the true vars
w <<- matrix(0, p, l)
for (j in 1:l) {
true_var <- c(1, 2, 3)  #sample(p, q)
w[true_var, j] <<- round(rnorm(q, mean = 0, sd = 1), 3)
}
y <<- X %*% w
# epsilon is the gaussian error. dimension: n * l.
epsilon <- matrix(rnorm(n * l), n, l)
y <<- y + epsilon
}
compare <- function() {
mt_fit = glmnet(X, y, family = "mgaussian")
#plot(mt_fit, type.coef = "2norm", label = TRUE)
## Use Cross Validation to select $\lambda$, and compare.
cv.mt_fit = cv.glmnet(X, y, family = "mgaussian", intercept = FALSE)
cv.fit1 = cv.glmnet(X, y[, 1], intercept = FALSE)
cv.fit2 = cv.glmnet(X, y[, 2], intercept = FALSE)
cv.fit3 = cv.glmnet(X, y[, 3], intercept = FALSE)
cv.fit4 = cv.glmnet(X, y[, 4], intercept = FALSE)
cv.fit5 = cv.glmnet(X, y[, 5], intercept = FALSE)
# Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold
# If grouped=FALSE, an error matrix is built up at the observation level from the predictions from the nfold fits, and then summarized
#plot(cv.mt_fit)
#plot(cv.fit1)
#plot(cv.fit2)
#plot(cv.fit3)
# multi-task
mt_beta = cbind(
coef(cv.mt_fit)$y1,
coef(cv.mt_fit)$y2,
coef(cv.mt_fit)$y3,
coef(cv.mt_fit)$y4,
coef(cv.mt_fit)$y5
)
mt_beta = mt_beta[-1,]
#print(mt_beta)
#print(sum((mt_beta - w) ^ 2))
# lasso for each response
lasso_beta = cbind(coef(cv.fit1),
coef(cv.fit2),
coef(cv.fit3),
coef(cv.fit4),
coef(cv.fit5))
lasso_beta = lasso_beta[-1,]
#print(lasso_beta)
#print(sum((lasso_beta - w) ^ 2))
# get whether correct selection
mt_correct <<- c(mt_correct, sum(as.logical(w) == as.logical(mt_beta)) / (p * l))
lasso_correct <<- c(lasso_correct, sum(as.logical(w) == as.logical(lasso_beta)) / (p * l))
# get RSS fotr w
lasso_wRSS <<- c(lasso_wRSS, sum((w - lasso_beta) ^ 2))
mt_wRSS <<- c(mt_wRSS, sum((w - mt_beta) ^ 2))
}
for(ii in 1:2){
getXy()
compare()
}
getXy()
compare()
set.seed(65587)
# (n=12, p=16) q in [1,3] 3, l = 5
n <- 12
p <- 16
l <- 5
q <- 3
X <- read.csv("design1226.csv", skip = 7, header = FALSE)
X <- apply(X, 1, gsub, pattern="\\+", replacement= "1", perl=TRUE)
X <- apply(X, 1, gsub, pattern="\\-", replacement= "-1", perl=TRUE)
X <- matrix(as.numeric(X), n, p)
lasso_correct <<- numeric()
mt_correct <<- numeric()
lasso_wRSS <<- numeric()
mt_wRSS <<- numeric()
set.seed(65587)
# (n=12, p=16) q in [1,3] 3, l = 5
n <- 12
p <- 16
l <- 5
q <- 3
X <- read.csv("design1226.csv", skip = 7, header = FALSE)
X <- apply(X, 1, gsub, pattern="\\+", replacement= "1", perl=TRUE)
X <- apply(X, 1, gsub, pattern="\\-", replacement= "-1", perl=TRUE)
X <- matrix(as.numeric(X), n, p)
lasso_correct <<- numeric()
mt_correct <<- numeric()
lasso_wRSS <<- numeric()
mt_wRSS <<- numeric()
getXy <- function(){
# Have 5 different responses
# There are 3 same true var for different tasks, the true vars are randomly chosen
# for every task, we need a vector to set the true vars
w <<- matrix(0, p, l)
for (j in 1:l) {
true_var <- c(1, 2, 3)  #sample(p, q)
w[true_var, j] <<- round(rnorm(q, mean = 0, sd = 1), 3)
}
y <<- X %*% w
# epsilon is the gaussian error. dimension: n * l.
epsilon <- matrix(rnorm(n * l), n, l)
y <<- y + epsilon
}
compare <- function() {
mt_fit = glmnet(X, y, family = "mgaussian")
#plot(mt_fit, type.coef = "2norm", label = TRUE)
## Use Cross Validation to select $\lambda$, and compare.
cv.mt_fit = cv.glmnet(X, y, family = "mgaussian", intercept = FALSE)
cv.fit1 = cv.glmnet(X, y[, 1], intercept = FALSE)
cv.fit2 = cv.glmnet(X, y[, 2], intercept = FALSE)
cv.fit3 = cv.glmnet(X, y[, 3], intercept = FALSE)
cv.fit4 = cv.glmnet(X, y[, 4], intercept = FALSE)
cv.fit5 = cv.glmnet(X, y[, 5], intercept = FALSE)
# Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold
# If grouped=FALSE, an error matrix is built up at the observation level from the predictions from the nfold fits, and then summarized
#plot(cv.mt_fit)
#plot(cv.fit1)
#plot(cv.fit2)
#plot(cv.fit3)
# multi-task
mt_beta = cbind(
coef(cv.mt_fit)$y1,
coef(cv.mt_fit)$y2,
coef(cv.mt_fit)$y3,
coef(cv.mt_fit)$y4,
coef(cv.mt_fit)$y5
)
mt_beta = mt_beta[-1,]
#print(mt_beta)
#print(sum((mt_beta - w) ^ 2))
# lasso for each response
lasso_beta = cbind(coef(cv.fit1),
coef(cv.fit2),
coef(cv.fit3),
coef(cv.fit4),
coef(cv.fit5))
lasso_beta = lasso_beta[-1,]
#print(lasso_beta)
#print(sum((lasso_beta - w) ^ 2))
# get whether correct selection
mt_correct <<- c(mt_correct, sum(as.logical(w) == as.logical(mt_beta)) / (p * l))
lasso_correct <<- c(lasso_correct, sum(as.logical(w) == as.logical(lasso_beta)) / (p * l))
# get RSS fotr w
lasso_wRSS <<- c(lasso_wRSS, sum((w - lasso_beta) ^ 2))
mt_wRSS <<- c(mt_wRSS, sum((w - mt_beta) ^ 2))
}
for(ii in 1:2){
getXy()
compare()
}
cv.fit1 = cv.glmnet(X, y[, 1], intercept = FALSE)
cv.fit2 = cv.glmnet(X, y[, 2], intercept = FALSE)
cv.fit3 = cv.glmnet(X, y[, 3], intercept = FALSE)
cv.fit4 = cv.glmnet(X, y[, 4], intercept = FALSE)
cv.fit5 = cv.glmnet(X, y[, 5], intercept = FALSE)
lasso_beta = cbind(coef(cv.fit1),
coef(cv.fit2),
coef(cv.fit3),
coef(cv.fit4),
coef(cv.fit5))
print(lasso_beta)
