compare <- function(ii) {
mt_fit = glmnet(X, y, family = "mgaussian")
#plot(mt_fit, type.coef = "2norm", label = TRUE)
## Use Cross Validation to select $\lambda$, and compare.
cv.fitall = cv.glmnet(X, g, nfolds = 12, intercept = FALSE)
cv.mt_fit = cv.glmnet(X, y, nfolds = 12, family = "mgaussian", intercept = FALSE)
cv.fit1 = cv.glmnet(X, y[, 1], nfolds = 12, intercept = FALSE)
cv.fit2 = cv.glmnet(X, y[, 2], nfolds = 12, intercept = FALSE)
cv.fit3 = cv.glmnet(X, y[, 3], nfolds = 12, intercept = FALSE)
cv.fit4 = cv.glmnet(X, y[, 4], nfolds = 12, intercept = FALSE)
cv.fit5 = cv.glmnet(X, y[, 5], nfolds = 12, intercept = FALSE)
# Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold
# If grouped=FALSE, an error matrix is built up at the observation level from the predictions from the nfold fits, and then summarized
#plot(cv.mt_fit)
#plot(cv.fit1)
#plot(cv.fit2)
#plot(cv.fit3)
# lasso for all
all_beta <- matrix(0, p, l)
for(i in 1:l){
all_beta[,i] = coef(cv.fitall)[-1,]
}
# multi-task
mt_beta <- matrix(0, p, l)
for(i in 1:l){
mt_beta[,i] = eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep='')))[-1,]
}
#####print(mt_beta)
# lasso for each response
lasso_beta <- matrix(0, p, l)
for(i in 1:l){
lasso_beta[,i] = eval(parse(text = paste('coef(cv.fit', i,')', sep='')))[-1,]
}
####print(lasso_beta)
# get whether correct selection, and sign correctness
# First one is selecting all the correct variable is correct
all_OC[ii] <<- sum(as.logical(b) == as.logical(all_beta)) == p*l
mt_OC[ii] <<- sum(as.logical(b) == as.logical(mt_beta)) == p*l
lasso_OC[ii] <<- sum(as.logical(b) == as.logical(lasso_beta)) == p * l
mt_SC[ii] <<- sum(sign(b) == sign(mt_beta)) == p*l
lasso_SC[ii] <<- sum(sign(b) == sign(lasso_beta)) == p*l
# Second one calculates the correct ratio in every rep
# mt_OC2 <<- c(mt_OC2, sum(as.logical(b) == as.logical(mt_beta)) / (p * l))
# lasso_OC2 <<- c(lasso_OC2, sum(as.logical(b) == as.logical(lasso_beta)) / (p * l))
#
# mt_SC2 <<- c(mt_SC2, sum(sign(b) == sign(mt_beta)) / (p * l))
# lasso_SC2 <<- c(lasso_SC2, sum(sign(b) == sign(lasso_beta)) / (p * l))
}
set.seed(87293)
for(ii in 1:n_rep){
getXy()
compare(ii)
}
#print(mt_OC)
#print(lasso_OC)
cat(mean(all_OC), mean(mt_OC), mean(lasso_OC), mean(mt_SC), mean(lasso_SC),'\n')#, mean(mt_OC2), mean(lasso_OC2), mean(mt_SC2), mean(lasso_SC2),'\n')
print(
cbind(
t.test(all_OC)$conf.int[1:2],
t.test(mt_OC)$conf.int[1:2],
t.test(lasso_OC)$conf.int[1:2],
t.test(mt_SC)$conf.int[1:2],
t.test(lasso_SC)$conf.int[1:2])
#t.test(mt_OC2)$conf.int[1:2],
# t.test(lasso_OC2)$conf.int[1:2],
# t.test(mt_SC2)$conf.int[1:2],
# t.test(lasso_SC2)$conf.int[1:2])
)
cv.fitall = cv.glmnet(X, g, nfolds = 5, intercept = FALSE)
all_beta <- matrix(0, p, l)
for(i in 1:l){
all_beta[,i] = coef(cv.fitall)[-1,]
}
View(all_beta)
mt_beta <- matrix(0, p, l)
for(i in 1:l){
mt_beta[,i] = eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep='')))[-1,]
}
#library(MASS)
library(glmnet)
cv.mt_fit = cv.glmnet(X, y, nfolds = 5, family = "mgaussian", intercept = FALSE)
mt_beta <- matrix(0, p, l)
for(i in 1:l){
mt_beta[,i] = eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep='')))[-1,]
}
View(mt_beta)
View(all_beta)
View(mt_beta)
View(all_beta)
View(b)
View(all_beta)
rowSums(b)
View(b)
View(y)
View(b)
b[1,1] = 5
b[1,4] =5
b[1,5] =3
b[16,1] =1
cv.mt_fit = cv.glmnet(X, y, nfolds = 5, family = "mgaussian", intercept = FALSE)
mt_beta <- matrix(0, p, l)
for(i in 1:l){
mt_beta[,i] = eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep='')))[-1,]
}
View(mt_beta)
y <<- X %*% b
epsilon <- matrix(rnorm(n * l), n, l)
y <<- y + epsilon
g <<- rowSums(y)
cv.fitall = cv.glmnet(X, g, nfolds = 5, intercept = FALSE)
cv.mt_fit = cv.glmnet(X, y, nfolds = 5, family = "mgaussian", intercept = FALSE)
b
all_beta <- matrix(0, p, l)
for(i in 1:l){
all_beta[,i] = coef(cv.fitall)[-1,]
}
for(i in 1:l){
mt_beta[,i] = eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep='')))[-1,]
}
View(all_beta)
View(mt_beta)
b[true_var, j] <<- -runif(1, 3, 5) #a  #runif(3, 3, 5)
true_var <- sample(p, q)
for (j in 1:l)
b[true_var, j] <<- -runif(1, 3, 5) #a  #runif(3, 3, 5)
b <<- matrix(0, p, l)
true_var <- sample(p, q)
for (j in 1:l)
b[true_var, j] <<- -runif(1, 3, 5) #a  #runif(3, 3, 5)
View(b)
getXy <- function(){
# Have 5 different responses
# There are 3 same true var for different tasks, the true vars are randomly chosen
# for every task, we need a vector to set the true vars
b <<- matrix(0, p, l)
if(SameTrue == TRUE){       #in this case, the variable is equal for every task
true_var <- sample(p, q)
for (j in 1:l)
b[true_var, j] <<- runif(1, 3, 5) #a  #runif(3, 3, 5)
}else{
for (j in 1:l){         #for each task, sample different true factors
true_var <- sample(p, q)
b[true_var, j] <<- a  #runif(3, 3, 5)
}
}
y <<- X %*% b
# epsilon is the gaussian error. dimension: n * l.
epsilon <- matrix(rnorm(n * l), n, l)
y <<- y + epsilon
g <<- rowSums(y)
}
compare <- function(ii) {
mt_fit = glmnet(X, y, family = "mgaussian")
#plot(mt_fit, type.coef = "2norm", label = TRUE)
## Use Cross Validation to select $\lambda$, and compare.
cv.fitall = cv.glmnet(X, g, nfolds = 5, intercept = FALSE)
cv.mt_fit = cv.glmnet(X, y, nfolds = 5, family = "mgaussian", intercept = FALSE)
cv.fit1 = cv.glmnet(X, y[, 1], nfolds = 5, intercept = FALSE)
cv.fit2 = cv.glmnet(X, y[, 2], nfolds = 5, intercept = FALSE)
cv.fit3 = cv.glmnet(X, y[, 3], nfolds = 5, intercept = FALSE)
cv.fit4 = cv.glmnet(X, y[, 4], nfolds = 5, intercept = FALSE)
cv.fit5 = cv.glmnet(X, y[, 5], nfolds = 5, intercept = FALSE)
# Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold
# If grouped=FALSE, an error matrix is built up at the observation level from the predictions from the nfold fits, and then summarized
#plot(cv.mt_fit)
#plot(cv.fit1)
#plot(cv.fit2)
#plot(cv.fit3)
# lasso for all
all_beta <- matrix(0, p, l)
for(i in 1:l){
all_beta[,i] = coef(cv.fitall)[-1,]
}
# multi-task
mt_beta <- matrix(0, p, l)
for(i in 1:l){
mt_beta[,i] = eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep='')))[-1,]
}
#####print(mt_beta)
# lasso for each response
lasso_beta <- matrix(0, p, l)
for(i in 1:l){
lasso_beta[,i] = eval(parse(text = paste('coef(cv.fit', i,')', sep='')))[-1,]
}
####print(lasso_beta)
# get whether correct selection, and sign correctness
# First one is selecting all the correct variable is correct
all_OC[ii] <<- sum(as.logical(b) == as.logical(all_beta)) == p*l
mt_OC[ii] <<- sum(as.logical(b) == as.logical(mt_beta)) == p*l
lasso_OC[ii] <<- sum(as.logical(b) == as.logical(lasso_beta)) == p * l
mt_SC[ii] <<- sum(sign(b) == sign(mt_beta)) == p*l
lasso_SC[ii] <<- sum(sign(b) == sign(lasso_beta)) == p*l
# Second one calculates the correct ratio in every rep
# mt_OC2 <<- c(mt_OC2, sum(as.logical(b) == as.logical(mt_beta)) / (p * l))
# lasso_OC2 <<- c(lasso_OC2, sum(as.logical(b) == as.logical(lasso_beta)) / (p * l))
#
# mt_SC2 <<- c(mt_SC2, sum(sign(b) == sign(mt_beta)) / (p * l))
# lasso_SC2 <<- c(lasso_SC2, sum(sign(b) == sign(lasso_beta)) / (p * l))
}
getXy()
View(b)
sample(c(-1,1), 1)
sample(c(-1,1), 1)
getXy <- function(){
# Have 5 different responses
# There are 3 same true var for different tasks, the true vars are randomly chosen
# for every task, we need a vector to set the true vars
b <<- matrix(0, p, l)
if(SameTrue == TRUE){       #in this case, the variable is equal for every task
true_var <- sample(p, q)
for(k in 1:q){
vsign = sample(c(-1,1), 1)
for (j in 1:l)
b[true_var, j] <<- vsign * runif(1, 3, 5) #a  #runif(3, 3, 5)
}
}else{
for (j in 1:l){         #for each task, sample different true factors
true_var <- sample(p, q)
b[true_var, j] <<- a  #runif(3, 3, 5)
}
}
y <<- X %*% b
# epsilon is the gaussian error. dimension: n * l.
epsilon <- matrix(rnorm(n * l), n, l)
y <<- y + epsilon
g <<- rowSums(y)
}
getXy()
View(b)
getXy <- function(){
# Have 5 different responses
# There are 3 same true var for different tasks, the true vars are randomly chosen
# for every task, we need a vector to set the true vars
b <<- matrix(0, p, l)
if(SameTrue == TRUE){       #in this case, the variable is equal for every task
true_var <- sample(p, q)
for(k in 1:q){
vsign = sample(c(-1,1), 1)
for (j in 1:l)
b[true_var[k], j] <<- vsign * runif(1, 3, 5) #a  #runif(3, 3, 5)
}
}else{
for (j in 1:l){         #for each task, sample different true factors
true_var <- sample(p, q)
b[true_var, j] <<- a  #runif(3, 3, 5)
}
}
y <<- X %*% b
# epsilon is the gaussian error. dimension: n * l.
epsilon <- matrix(rnorm(n * l), n, l)
y <<- y + epsilon
g <<- rowSums(y)
}
getXy()
View(b)
n_rep <- 200 #500 rep is quite long
a <- 5  # absolute value of true factor
SameTrue <- TRUE
#-------------------------
n <- 12
p <- 16
l <- 5
q <- 2  # number of true variables
X <- read.csv("design1226.csv", skip = 7, header = FALSE)
X <- apply(X, 1, gsub, pattern="\\+", replacement= "1", perl=TRUE)
X <- apply(X, 1, gsub, pattern="\\-", replacement= "-1", perl=TRUE)
X <- matrix(as.numeric(X), n, p)
all_OC <<- numeric(n_rep)
lasso_OC <<- numeric(n_rep)
mt_OC <<- numeric(n_rep)
lasso_SC <<- numeric(n_rep)
mt_SC <<- numeric(n_rep)
# lasso_OC2 <<- numeric()
# mt_OC2 <<- numeric()
# lasso_SC2 <<- numeric()
# mt_SC2 <<- numeric()
getXy <- function(){
# Have 5 different responses
# There are 3 same true var for different tasks, the true vars are randomly chosen
# for every task, we need a vector to set the true vars
b <<- matrix(0, p, l)
if(SameTrue == TRUE){       #in this case, the variable is equal for every task
true_var <- sample(p, q)
for(k in 1:q){
vsign = sample(c(-1,1), 1)
for (j in 1:l)
b[true_var[k], j] <<- vsign * runif(1, 3, 5) #a  #runif(3, 3, 5)
}
}else{
for (j in 1:l){         #for each task, sample different true factors
true_var <- sample(p, q)
b[true_var, j] <<- a  #runif(3, 3, 5)
}
}
y <<- X %*% b
# epsilon is the gaussian error. dimension: n * l.
epsilon <- matrix(rnorm(n * l), n, l)
y <<- y + epsilon
g <<- rowSums(y)
}
compare <- function(ii) {
mt_fit = glmnet(X, y, family = "mgaussian")
#plot(mt_fit, type.coef = "2norm", label = TRUE)
## Use Cross Validation to select $\lambda$, and compare.
cv.fitall = cv.glmnet(X, g, nfolds = 5, intercept = FALSE)
cv.mt_fit = cv.glmnet(X, y, nfolds = 5, family = "mgaussian", intercept = FALSE)
cv.fit1 = cv.glmnet(X, y[, 1], nfolds = 5, intercept = FALSE)
cv.fit2 = cv.glmnet(X, y[, 2], nfolds = 5, intercept = FALSE)
cv.fit3 = cv.glmnet(X, y[, 3], nfolds = 5, intercept = FALSE)
cv.fit4 = cv.glmnet(X, y[, 4], nfolds = 5, intercept = FALSE)
cv.fit5 = cv.glmnet(X, y[, 5], nfolds = 5, intercept = FALSE)
# Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold
# If grouped=FALSE, an error matrix is built up at the observation level from the predictions from the nfold fits, and then summarized
#plot(cv.mt_fit)
#plot(cv.fit1)
#plot(cv.fit2)
#plot(cv.fit3)
# lasso for all
all_beta <- matrix(0, p, l)
for(i in 1:l){
all_beta[,i] = coef(cv.fitall)[-1,]
}
# multi-task
mt_beta <- matrix(0, p, l)
for(i in 1:l){
mt_beta[,i] = eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep='')))[-1,]
}
#####print(mt_beta)
# lasso for each response
lasso_beta <- matrix(0, p, l)
for(i in 1:l){
lasso_beta[,i] = eval(parse(text = paste('coef(cv.fit', i,')', sep='')))[-1,]
}
####print(lasso_beta)
# get whether correct selection, and sign correctness
# First one is selecting all the correct variable is correct
all_OC[ii] <<- sum(as.logical(b) == as.logical(all_beta)) == p*l
mt_OC[ii] <<- sum(as.logical(b) == as.logical(mt_beta)) == p*l
lasso_OC[ii] <<- sum(as.logical(b) == as.logical(lasso_beta)) == p * l
mt_SC[ii] <<- sum(sign(b) == sign(mt_beta)) == p*l
lasso_SC[ii] <<- sum(sign(b) == sign(lasso_beta)) == p*l
# Second one calculates the correct ratio in every rep
# mt_OC2 <<- c(mt_OC2, sum(as.logical(b) == as.logical(mt_beta)) / (p * l))
# lasso_OC2 <<- c(lasso_OC2, sum(as.logical(b) == as.logical(lasso_beta)) / (p * l))
#
# mt_SC2 <<- c(mt_SC2, sum(sign(b) == sign(mt_beta)) / (p * l))
# lasso_SC2 <<- c(lasso_SC2, sum(sign(b) == sign(lasso_beta)) / (p * l))
}
set.seed(87293)
for(ii in 1:n_rep){
getXy()
compare(ii)
}
#print(mt_OC)
#print(lasso_OC)
cat(mean(all_OC), mean(mt_OC), mean(lasso_OC), mean(mt_SC), mean(lasso_SC),'\n')#, mean(mt_OC2), mean(lasso_OC2), mean(mt_SC2), mean(lasso_SC2),'\n')
print(
cbind(
t.test(all_OC)$conf.int[1:2],
t.test(mt_OC)$conf.int[1:2],
t.test(lasso_OC)$conf.int[1:2],
t.test(mt_SC)$conf.int[1:2],
t.test(lasso_SC)$conf.int[1:2])
#t.test(mt_OC2)$conf.int[1:2],
# t.test(lasso_OC2)$conf.int[1:2],
# t.test(mt_SC2)$conf.int[1:2],
# t.test(lasso_SC2)$conf.int[1:2])
)
View(b)
getXy <- function(){
# Have 5 different responses
# There are 3 same true var for different tasks, the true vars are randomly chosen
# for every task, we need a vector to set the true vars
b <<- matrix(0, p, l)
if(SameTrue == TRUE){       #in this case, the variable is equal for every task
true_var <- sample(p, q)
for(k in 1:q){
for (j in 1:l){
vsign = sample(c(-1,1), 1)
b[true_var[k], j] <<- vsign * runif(1, 3, 5) #a  #runif(3, 3, 5)
}
}
}else{
for (j in 1:l){         #for each task, sample different true factors
true_var <- sample(p, q)
b[true_var, j] <<- a  #runif(3, 3, 5)
}
}
y <<- X %*% b
# epsilon is the gaussian error. dimension: n * l.
epsilon <- matrix(rnorm(n * l), n, l)
y <<- y + epsilon
g <<- rowSums(y)
}
compare <- function(ii) {
mt_fit = glmnet(X, y, family = "mgaussian")
#plot(mt_fit, type.coef = "2norm", label = TRUE)
## Use Cross Validation to select $\lambda$, and compare.
cv.fitall = cv.glmnet(X, g, nfolds = 5, intercept = FALSE)
cv.mt_fit = cv.glmnet(X, y, nfolds = 5, family = "mgaussian", intercept = FALSE)
cv.fit1 = cv.glmnet(X, y[, 1], nfolds = 5, intercept = FALSE)
cv.fit2 = cv.glmnet(X, y[, 2], nfolds = 5, intercept = FALSE)
cv.fit3 = cv.glmnet(X, y[, 3], nfolds = 5, intercept = FALSE)
cv.fit4 = cv.glmnet(X, y[, 4], nfolds = 5, intercept = FALSE)
cv.fit5 = cv.glmnet(X, y[, 5], nfolds = 5, intercept = FALSE)
# Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold
# If grouped=FALSE, an error matrix is built up at the observation level from the predictions from the nfold fits, and then summarized
#plot(cv.mt_fit)
#plot(cv.fit1)
#plot(cv.fit2)
#plot(cv.fit3)
# lasso for all
all_beta <- matrix(0, p, l)
for(i in 1:l){
all_beta[,i] = coef(cv.fitall)[-1,]
}
# multi-task
mt_beta <- matrix(0, p, l)
for(i in 1:l){
mt_beta[,i] = eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep='')))[-1,]
}
#####print(mt_beta)
# lasso for each response
lasso_beta <- matrix(0, p, l)
for(i in 1:l){
lasso_beta[,i] = eval(parse(text = paste('coef(cv.fit', i,')', sep='')))[-1,]
}
####print(lasso_beta)
# get whether correct selection, and sign correctness
# First one is selecting all the correct variable is correct
all_OC[ii] <<- sum(as.logical(b) == as.logical(all_beta)) == p*l
mt_OC[ii] <<- sum(as.logical(b) == as.logical(mt_beta)) == p*l
lasso_OC[ii] <<- sum(as.logical(b) == as.logical(lasso_beta)) == p * l
mt_SC[ii] <<- sum(sign(b) == sign(mt_beta)) == p*l
lasso_SC[ii] <<- sum(sign(b) == sign(lasso_beta)) == p*l
# Second one calculates the correct ratio in every rep
# mt_OC2 <<- c(mt_OC2, sum(as.logical(b) == as.logical(mt_beta)) / (p * l))
# lasso_OC2 <<- c(lasso_OC2, sum(as.logical(b) == as.logical(lasso_beta)) / (p * l))
#
# mt_SC2 <<- c(mt_SC2, sum(sign(b) == sign(mt_beta)) / (p * l))
# lasso_SC2 <<- c(lasso_SC2, sum(sign(b) == sign(lasso_beta)) / (p * l))
}
set.seed(87293)
for(ii in 1:n_rep){
getXy()
compare(ii)
}
#print(mt_OC)
#print(lasso_OC)
cat(mean(all_OC), mean(mt_OC), mean(lasso_OC), mean(mt_SC), mean(lasso_SC),'\n')#, mean(mt_OC2), mean(lasso_OC2), mean(mt_SC2), mean(lasso_SC2),'\n')
print(
cbind(
t.test(all_OC)$conf.int[1:2],
t.test(mt_OC)$conf.int[1:2],
t.test(lasso_OC)$conf.int[1:2],
t.test(mt_SC)$conf.int[1:2],
t.test(lasso_SC)$conf.int[1:2])
#t.test(mt_OC2)$conf.int[1:2],
# t.test(lasso_OC2)$conf.int[1:2],
# t.test(mt_SC2)$conf.int[1:2],
# t.test(lasso_SC2)$conf.int[1:2])
)
View(X)
View(b)
b[16,1] = 100
y <<- X %*% b
epsilon <- matrix(rnorm(n * l), n, l)
y <<- y + epsilon
g <<- rowSums(y)
cv.mt_fit = cv.glmnet(X, y, nfolds = 5, family = "mgaussian", intercept = FALSE)
mt_beta <- matrix(0, p, l)
for(i in 1:l){
mt_beta[,i] = eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep='')))[-1,]
}
View(mt_beta)
b[2,]=c(0,0,0,0,0)
b[1,5]=10000000
y <<- X %*% b
y <<- y + epsilon
cv.mt_fit = cv.glmnet(X, y, nfolds = 5, family = "mgaussian", intercept = FALSE)
for(i in 1:l){
mt_beta[,i] = eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep='')))[-1,]
}
y <<- X %*% b
cv.mt_fit = cv.glmnet(X, y, nfolds = 5, family = "mgaussian", intercept = FALSE)
for(i in 1:l){
mt_beta[,i] = eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep='')))[-1,]
}
y <<- X %*% b
epsilon <- matrix(rnorm(n * l), n, l)
y <<- y + 0.01*epsilon
cv.mt_fit = cv.glmnet(X, y, nfolds = 5, family = "mgaussian", intercept = FALSE)
for(i in 1:l){
mt_beta[,i] = eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep='')))[-1,]
}
y <<- X %*% b
y <<- y + 0.000001*epsilon
cv.mt_fit = cv.glmnet(X, y, nfolds = 5, family = "mgaussian", intercept = FALSE)
for(i in 1:l){
mt_beta[,i] = eval(parse(text = paste('coef(cv.mt_fit)$y', i, sep='')))[-1,]
}
View(b)
X[,c(15,16)]
XA = X[,c(15,16)]
t(XA)*XA
t(XA)
XA
t(XA)%*%XA
inv(t(XA)%*%XA)
solve(t(XA)%*%XA)
inv=solve(t(XA)%*%XA)
t(XA)%*%XA%*%inv
